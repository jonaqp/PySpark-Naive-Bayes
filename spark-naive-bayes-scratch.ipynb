{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos = sc.textFile('NB_files/train_pos.txt')\n",
    "train_neg = sc.textFile('NB_files/train_neg.txt')\n",
    "test_pos = sc.textFile('NB_files/test_pos.txt')\n",
    "test_neg = sc.textFile('NB_files/test_neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['a', 'able', 'about', 'across', 'after', 'all', 'almost', 'also',\n",
    "              'am', 'among', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because',\n",
    "              'been', 'but', 'by', 'can', 'cannot', 'could', 'dear',\n",
    "              'did', 'do', 'does', 'either', 'else', 'ever', 'every', 'for',\n",
    "              'from', 'get', 'got', 'had', 'has', 'have', 'he', 'her', 'hers',\n",
    "              'him', 'his', 'how', 'however', 'i', 'if', 'in', 'into', 'is',\n",
    "              'it', 'its', 'just', 'least', 'let', 'like', 'likely', 'may',\n",
    "              'me', 'might', 'most', 'must', 'my', 'neither', 'no', 'nor',\n",
    "              'not', 'of', 'off', 'often', 'on', 'only', 'or', 'other', 'our',\n",
    "              'own', 'rather', 'said', 'say', 'says', 'she', 'should', 'since',\n",
    "              'so', 'some', 'than', 'that', 'the', 'their', 'them', 'then',\n",
    "              'there', 'these', 'they', 'this', 'tis', 'to', 'too', 'twas', 'us',\n",
    "              've', 'wants', 'was', 'we', 'were', 'what', 'when', 'where', 'which',\n",
    "              'while', 'who', 'whom', 'why', 'will', 'with', 'would', 'yet',\n",
    "              'you', 'your']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(blob):\n",
    "    blob = blob.split('\\n')\n",
    "    blob = [re.sub('[^A-Za-z]', ' ', i) for i in blob]\n",
    "    blob = [i.split() for i in blob]\n",
    "    blob = [item for sublist in blob for item in sublist]\n",
    "    blob = [word.strip().lower() for word in blob if word not in stop_words if len(word) >= 3]\n",
    "    return blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten the list for total word count and \n",
    "train_pos_bag = train_pos.flatMap(parse)\n",
    "train_neg_bag = train_neg.flatMap(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count of All Words per Class\n",
    "pos_word_count = train_pos_bag.count()\n",
    "neg_word_count = train_neg_bag.count()\n",
    "\n",
    "# Count per Word\n",
    "pos_count_by_word = train_pos_bag.countByValue()\n",
    "neg_count_by_word = train_neg_bag.countByValue()\n",
    "\n",
    "# Unique Words in All Classes\n",
    "unique_words_pos = [i for i in pos_count_by_word.keys() if type(i) is not int]\n",
    "unique_words_neg = [i for i in neg_count_by_word.keys() if type(i) is not int]\n",
    "\n",
    "num_unique = len(unique_words_pos) + len(unique_words_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Naive Bayes Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def the_naive_bassifier(label_tuple):\n",
    "    '''\n",
    "    input: Spark RDD tuple of a class and a word list from document\n",
    "    output: predicted class and actual class labels\n",
    "    '''\n",
    "    pos_list = list()\n",
    "    neg_list = list()\n",
    "\n",
    "    orig_label = label_tuple[0]\n",
    "    new_text = Counter(label_tuple[1])\n",
    "    test_doc_wc = len(new_text)\n",
    "    \n",
    "    for i in new_text.keys():\n",
    "        try:   \n",
    "            cnt_word_pos = pos_count_by_word[i]\n",
    "        except KeyError:\n",
    "            cnt_word_pos = 0\n",
    "\n",
    "        try:\n",
    "            cnt_word_neg = neg_count_by_word[i]\n",
    "        except KeyError:\n",
    "            cnt_word_neg = 0\n",
    "\n",
    "        cl_pos = (cnt_word_pos + 1) / float(pos_word_count + num_unique)\n",
    "        cl_neg = (cnt_word_neg + 1) / float(neg_word_count + num_unique)\n",
    "        \n",
    "        pos_list.append(cl_pos ** new_text[i])\n",
    "        neg_list.append(cl_neg ** new_text[i])\n",
    "        \n",
    "    pos_score = np.log(.5) + np.sum(pos_list)\n",
    "    neg_score = np.log(.5) + np.sum(neg_list)\n",
    "        \n",
    "    if pos_score > neg_score:\n",
    "        pred_label = 1.\n",
    "    elif pos_score == neg_score:\n",
    "        pred_label = np.random.randint(0, 2)\n",
    "    else:\n",
    "        pred_label = 0\n",
    "\n",
    "    return (pred_label, orig_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse into Bags of Words\n",
    "test_pos_bag = test_pos.map(parse)\n",
    "test_neg_bag = test_neg.map(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Label\n",
    "test_pos_bag = test_pos_bag.map(lambda x: (1., x))\n",
    "test_neg_bag = test_neg_bag.map(lambda x: (0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnionRDD[14] at union at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join\n",
    "join_test = test_pos_bag.union(test_neg_bag)\n",
    "join_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[15] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rdd = join_test.map(the_naive_bassifier)\n",
    "pred_rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69112"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = 1.0 * pred_rdd.filter(lambda (x, v): x == v).count() / pred_rdd.count()\n",
    "test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
